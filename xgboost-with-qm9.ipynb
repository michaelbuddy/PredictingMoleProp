{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# 导入一些库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../input'\n",
    "SUBMISSIONS_PATH = './'\n",
    "# 使用原子的序号来对表示这些原子\n",
    "ATOMIC_NUMBERS = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import os\n",
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集的数据类型设置\n",
    "train_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index_0': 'int8',\n",
    "    'atom_index_1': 'int8',\n",
    "    'type': 'category',\n",
    "    'scalar_coupling_constant': 'float32'\n",
    "}\n",
    "# 读取训练集文件\n",
    "train_csv = pd.read_csv(f'{DATA_PATH}/champs-scalar-coupling/train.csv', index_col='id', dtype=train_dtypes)\n",
    "# 将molecule_name的格式从dsgdb9nsd_xx改成xx方面处理\n",
    "train_csv['molecule_index'] = train_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "train_csv = train_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type', 'scalar_coupling_constant']]\n",
    "#打印前10个元素\n",
    "train_csv.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取需要提交的文件\n",
    "submission_csv = pd.read_csv(f'{DATA_PATH}/champs-scalar-coupling/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取测试文件\n",
    "test_csv = pd.read_csv(f'{DATA_PATH}/champs-scalar-coupling/test.csv', index_col='id', dtype=train_dtypes)\n",
    "test_csv['molecule_index'] = test_csv['molecule_name'].str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "test_csv = test_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\n",
    "test_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结构数据类型\n",
    "structures_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index': 'int8',\n",
    "    'atom': 'category',\n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32'\n",
    "}\n",
    "structures_csv = pd.read_csv(f'{DATA_PATH}/champs-scalar-coupling/structures.csv', dtype=structures_dtypes)\n",
    "structures_csv['molecule_index'] = structures_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "structures_csv = structures_csv[['molecule_index', 'atom_index', 'atom', 'x', 'y', 'z']]\n",
    "structures_csv['atom'] = structures_csv['atom'].replace(ATOMIC_NUMBERS).astype('int8')\n",
    "structures_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 降低内存使用，感觉每个变量的取值范围将其动态改变类型\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将Categorical类型的数据变成热编码数据\n",
    "def dummies(df, list_cols):\n",
    "    for col in list_cols:\n",
    "        df_dummies = pd.get_dummies(df[col], drop_first=True, \n",
    "                                    prefix=(str(col)))\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "# 添加QM9特征\n",
    "def add_qm9_features(df):\n",
    "    # 读取qm9数据\n",
    "    data_qm9 = pd.read_pickle('../input/quantum-machine-9-qm9/data.covs.pickle')\n",
    "    # 抛去一些无用的和重复的特征\n",
    "    to_drop = ['type', \n",
    "               'linear', \n",
    "               'atom_index_0', \n",
    "               'atom_index_1', \n",
    "               'scalar_coupling_constant', \n",
    "               'U', 'G', 'H', \n",
    "               'mulliken_mean', 'r2', 'U0']\n",
    "    data_qm9 = data_qm9.drop(columns = to_drop, axis=1)\n",
    "    # 减少内存\n",
    "    data_qm9 = reduce_mem_usage(data_qm9,verbose=False)\n",
    "    # 将molecule_index改成\n",
    "    data_qm9['molecule_index'] = data_qm9.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "    data_qm9=data_qm9.drop(columns=['molecule_name'])\n",
    "    # 将qm9特征加入到df里面去\n",
    "    df = pd.merge(df, data_qm9, how='left', on=['molecule_index','id'])\n",
    "    # 抛去molecule_index,id这个对我们预测没啥帮助，此时df已经是最后的训练集或者测试集，特征工程已经处理结束\n",
    "    df=df.drop(columns=['molecule_index','id'])\n",
    "    del data_qm9\n",
    "    gc.collect()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Distance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 根据耦合键的类型，提取特定的数据\n",
    "def build_type_dataframes(base, structures, coupling_type):\n",
    "    base = base[base['type'] == coupling_type].drop('type', axis=1).copy()\n",
    "    base = base.reset_index()\n",
    "    base['id'] = base['id'].astype('int32')\n",
    "    structures = structures[structures['molecule_index'].isin(base['molecule_index'])]\n",
    "    return base, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从structure根据molecule_index和atom_index来得到得到atom的坐标\n",
    "def add_coordinates(base, structures, index):\n",
    "    df = pd.merge(base, structures, how='inner',\n",
    "                  left_on=['molecule_index', f'atom_index_{index}'],\n",
    "                  right_on=['molecule_index', 'atom_index']).drop(['atom_index'], axis=1)\n",
    "    df = df.rename(columns={\n",
    "        'atom': f'atom_{index}',\n",
    "        'x': f'x_{index}',\n",
    "        'y': f'y_{index}',\n",
    "        'z': f'z_{index}'\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 添加原子的信息\n",
    "def add_atoms(base, atoms):\n",
    "    df = pd.merge(base, atoms, how='inner',\n",
    "                  on=['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 除了原有的atom_index_0和atom_index_1的那些行不加进去，其它的都加进去\n",
    "def merge_all_atoms(base, structures):\n",
    "    df = pd.merge(base, structures, how='left',\n",
    "                  left_on=['molecule_index'],\n",
    "                  right_on=['molecule_index'])\n",
    "    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到中心点的坐标\n",
    "def add_center(df):\n",
    "    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n",
    "    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n",
    "    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n",
    "\n",
    "# 得到到中心点的距离\n",
    "def add_distance_to_center(df):\n",
    "    df['d_c'] = ((\n",
    "        (df['x_c'] - df['x'])**np.float32(2) +\n",
    "        (df['y_c'] - df['y'])**np.float32(2) + \n",
    "        (df['z_c'] - df['z'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "# 计算下标suffix1,和suffix2之间距离\n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算各个原子间的距离\n",
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 增加一个特征，该特征为molecule_index的分子对应原子的个数\n",
    "def add_n_atoms(base, structures):\n",
    "    dfs = structures['molecule_index'].value_counts().rename('n_atoms').to_frame()\n",
    "    return pd.merge(base, dfs, left_on='molecule_index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 根据耦合类型来提取特定的数据，进行小测试\n",
    "coupling_type='1JHC'\n",
    "base, structures = build_type_dataframes(train_csv, structures_csv, coupling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base的头部\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuctures的头部\n",
    "structures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加第一个原子的坐标\n",
    "base = add_coordinates(base, structures, 0)\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加第二个原子的坐标\n",
    "base = add_coordinates(base, structures, 1)\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建需要跑的数据集\n",
    "def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n",
    "    # 得到base，structures\n",
    "    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n",
    "    # 添加原子1，原子2的坐标\n",
    "    base = add_coordinates(base, structures, 0)\n",
    "    base = add_coordinates(base, structures, 1)\n",
    "    # 扔掉原子1，2的序号的两列\n",
    "    base = base.drop(['atom_0', 'atom_1'], axis=1)\n",
    "    #  扔掉id这一列\n",
    "    atoms = base.drop('id', axis=1).copy()\n",
    "    \n",
    "    # 如果有scalar_coupling_constant这一列，则丢掉，scalar_coupling_constant这列是y的值\n",
    "    if 'scalar_coupling_constant' in some_csv:\n",
    "        atoms = atoms.drop(['scalar_coupling_constant'], axis=1)\n",
    "        \n",
    "    # 添加中心点\n",
    "    add_center(atoms)\n",
    "    \n",
    "    # 删掉原子1，原子2的坐标，现在用中心点来替代\n",
    "    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n",
    "\n",
    "    # 合并所有的原子\n",
    "    atoms = merge_all_atoms(atoms, structures)\n",
    "    \n",
    "    # 对所有的原子添加到中心的距离\n",
    "    add_distance_to_center(atoms)\n",
    "    \n",
    "    # 删除中心点位置\n",
    "    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n",
    "    \n",
    "    # 按照molecule_index,atom_index_0,atom_index_1,d_c来对atoms进行排序\n",
    "    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n",
    "    \n",
    "    # 提取原子小于n_atoms的分子\n",
    "    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_c'], axis=1)\n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "\n",
    "    # 对索引设置并通过molecule_index展开\n",
    "    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()\n",
    "    \n",
    "    # 转回int8的类型\n",
    "    for col in atoms.columns:\n",
    "        if col.startswith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "    # 转类型\n",
    "    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n",
    "    \n",
    "    # 添加原子信息\n",
    "    full = add_atoms(base, atoms)\n",
    "    \n",
    "    # 添加距离\n",
    "    add_distances(full)\n",
    "    \n",
    "    # 根据id来进行重新排序\n",
    "    full.sort_values('id', inplace=True)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成需要使用的label\n",
    "def take_n_atoms(df, n_atoms, four_start=4):\n",
    "    labels = []\n",
    "    for i in range(2, n_atoms):\n",
    "        label = f'atom_{i}'\n",
    "        labels.append(label)\n",
    "\n",
    "    for i in range(n_atoms):\n",
    "        num = min(i, 4) if i < four_start else 4\n",
    "        for j in range(num):\n",
    "            labels.append(f'd_{i}_{j}')\n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        labels.append('scalar_coupling_constant')\n",
    "    labels=['id','molecule_index']+labels\n",
    "    return df[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check XGBOOST with the smallest type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't calculate distances for `d_0_x`, `d_1_1`, `d_2_2`, `d_2_3`, `d_3_3` because we already have them in later atoms(`d_0_1` == `d_1_0`) or they are equal to zeros(e.g. `d_1_1`, `d_2_2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成x,y数据\n",
    "def build_x_y_data(some_csv, coupling_type, n_atoms):\n",
    "    full = build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=n_atoms)\n",
    "    df = take_n_atoms(full, n_atoms)\n",
    "    df = df.fillna(0)\n",
    "    df=add_qm9_features(df)\n",
    "    print(df.columns)\n",
    "    \n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        X_data = df.drop(['scalar_coupling_constant'], axis=1).values.astype('float32')\n",
    "        y_data = df['scalar_coupling_constant'].values.astype('float32')\n",
    "    else:\n",
    "        X_data = df.values.astype('float32')\n",
    "        y_data = None\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拿出1JHN进行一个测试\n",
    "tx,ty=build_x_y_data(train_csv, '1JHN', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgb的参数\n",
    "xgb_params = {'eta': 0.1,\n",
    "              'max_depth': 11,\n",
    "              'subsample': 0.9,\n",
    "              'objective': 'reg:linear',\n",
    "              'eval_metric': 'mae',\n",
    "              'tree_method':'gpu_hist',\n",
    "              'colsample_bytree':1,\n",
    "              'alpha':0.2,\n",
    "              'lambda':0.1,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_predict_for_one_coupling_type(coupling_type, submission,oof_sub,n_atoms, n_folds=5,n_depth=9, n_splits=5, random_state=129):\n",
    "    print(f'*** Training Model for {coupling_type} ***')\n",
    "    # 训练集\n",
    "    X_data, y_data = build_x_y_data(train_csv, coupling_type, n_atoms)\n",
    "    # 测试集\n",
    "    X_test, _ = build_x_y_data(test_csv, coupling_type, n_atoms)\n",
    "    # 需要提交的数据\n",
    "    y_pred = np.zeros(X_test.shape[0], dtype='float32')\n",
    "    # oof本来使用了stacking的，后来没用上\n",
    "    oof = np.zeros(X_data.shape[0],dtype='float32')\n",
    "    # 交叉验证的分数\n",
    "    cv_score = 0\n",
    "    # K折交叉验证\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X_data, y_data)):\n",
    "        X_train, X_val = X_data[train_index], X_data[val_index]\n",
    "        y_train, y_val = y_data[train_index], y_data[val_index]\n",
    "        # 转化为DMatrix这样xgboost能用\n",
    "        train_data=xgb.DMatrix(data=X_train,label=y_train)\n",
    "        vaild_data=xgb.DMatrix(data=X_val,label=y_val)\n",
    "        xgb_params['max_depth']=n_depth\n",
    "        # 设置观测的指标\n",
    "        watchlist = [(vaild_data, 'valid_data')]\n",
    "        \n",
    "        # iterations最多迭代次数，每个类别不一样\n",
    "        if coupling_type in ['1JHN']:\n",
    "            iterations=5000\n",
    "        if coupling_type == '1JHC':\n",
    "            iterations=6000\n",
    "        else:\n",
    "            iterations=4700\n",
    "            \n",
    "        # xgb开始训练\n",
    "        model = xgb.train(dtrain=train_data, num_boost_round=iterations, evals=watchlist, early_stopping_rounds=250,params=xgb_params,verbose_eval=500)\n",
    "        # xgb对验证集的预测结果\n",
    "        y_val_pred=model.predict(xgb.DMatrix(X_val))\n",
    "        # 得到验证集的分数\n",
    "        val_score = np.log(mean_absolute_error(y_val, y_val_pred))\n",
    "        oof[val_index]=y_val_pred\n",
    "        print(f'{coupling_type} Fold {fold}, logMAE: {val_score}')\n",
    "        cv_score += val_score / n_folds\n",
    "        # xgb对测试集的预测结果\n",
    "        y_pred += model.predict(xgb.DMatrix(X_test)) / n_folds\n",
    "\n",
    "        \n",
    "    submission.loc[test_csv['type'] == coupling_type, 'scalar_coupling_constant'] = y_pred\n",
    "    oof_sub.loc[train_csv['type'] == coupling_type, 'scalar_coupling_constant'] = oof\n",
    "    return cv_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的n_atoms个数设置\n",
    "model_params = {\n",
    "    '1JHN': 10,\n",
    "    '1JHC': 13,\n",
    "    '2JHH': 10,\n",
    "    '2JHN': 10,\n",
    "    '2JHC': 10,\n",
    "    '3JHH': 10,\n",
    "    '3JHC': 10,\n",
    "    '3JHN': 10\n",
    "}\n",
    "# 模型的深度设置\n",
    "model_depth=\\\n",
    "{\n",
    "    '1JHN': 11,\n",
    "    '1JHC': 11,\n",
    "    '2JHH': 11,\n",
    "    '2JHN': 11,\n",
    "    '2JHC': 11,\n",
    "    '3JHH': 11,\n",
    "    '3JHC': 11,\n",
    "    '3JHN': 11\n",
    "}\n",
    "# 7折交叉验证\n",
    "N_FOLDS = 7\n",
    "# \n",
    "submission = submission_csv.copy()\n",
    "oof_submission=train_csv[['type','scalar_coupling_constant']].copy()\n",
    "oof_submission['scalar_coupling_constant']=0\n",
    "\n",
    "# 对于每种类型都进行训练\n",
    "cv_scores = {}\n",
    "for coupling_type in model_params.keys():\n",
    "    cv_score = train_and_predict_for_one_coupling_type(\n",
    "        coupling_type, submission,oof_submission,n_atoms=model_params[coupling_type], n_folds=N_FOLDS,n_depth=model_depth[coupling_type],random_state=130)\n",
    "    cv_scores[coupling_type] = cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oof_submission.head()\n",
    "oof_submission.to_csv('xgb_oof.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'type': list(cv_scores.keys()), 'cv_score': list(cv_scores.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印交叉验证的分数\n",
    "np.mean(list(cv_scores.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 提交最终结果\n",
    "submission.to_csv(f'{SUBMISSIONS_PATH}/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Room for improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
